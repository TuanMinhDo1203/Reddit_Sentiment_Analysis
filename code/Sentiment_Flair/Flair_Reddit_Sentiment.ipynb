{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "from emoji import demojize\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Manager\n",
    "\n",
    "\n",
    "# Load Flair's pre-trained sentiment classifier\n",
    "print(\"Loading Flair model...\")\n",
    "start_time = time.time()\n",
    "classifier = TextClassifier.load('en-sentiment')\n",
    "classifier.to('cpu')  # Explicitly use CPU\n",
    "print(f\"Model loaded in {time.time() - start_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "usecols = ['comment_id', 'comment_text', 'comment_author', 'comment_time', 'comment_score', 'match_time', 'matchday', 'home_team', 'away_team', 'winner']\n",
    "df = pd.read_csv('comments_data.csv', usecols=usecols, dtype={'comment_id': 'object', 'comment_score': 'float64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Inspecting comment_score column:\")\n",
    "print(df['comment_score'].head(10))  # Show first 10 values\n",
    "print(\"Data type:\", df['comment_score'].dtype)\n",
    "print(\"Number of non-numeric values (NaN):\", df['comment_score'].isna().sum())\n",
    "print(\"Unique values in comment_score:\", df['comment_score'].unique()[:20])  # Show first 20 unique values\n",
    "\n",
    "# Convert comment_score to int32 after loading\n",
    "df['comment_score'] = pd.to_numeric(df['comment_score'], errors='coerce').astype('int32', errors='ignore')\n",
    "print(f\"Loaded dataset with {len(df)} rows in {time.time() - start_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom mapping for emoticons or emojis not handled by demojize\n",
    "custom_emote_map = {\n",
    "    ':-)': ':smiley:',\n",
    "    ':)': ':smiley:',\n",
    "    ':-(': ':frowning_face:',\n",
    "    ':(': ':frowning_face:',\n",
    "    '>:(': ':angry_face:',\n",
    "    # Add more as needed based on your dataset\n",
    "}\n",
    "\n",
    "def preprocess_emotes_batch(comments):\n",
    "    # Vectorized preprocessing to convert emojis to text descriptions\n",
    "    comments = comments.apply(lambda x: demojize(x) if pd.notna(x) else x)\n",
    "    # Apply custom mappings\n",
    "    for emote, desc in custom_emote_map.items():\n",
    "        comments = comments.str.replace(emote, desc, regex=False)\n",
    "    return comments\n",
    "\n",
    "# Function to get sentiment using Flair with three classes, emoji preprocessing, and slang override\n",
    "def get_flair_sentiment(text, neutral_threshold=0.6):\n",
    "    if pd.isna(text) or text.strip() == '[deleted]':\n",
    "        return None, None\n",
    "    # Preprocess text to convert emotes (single text version for consistency)\n",
    "    processed_text = preprocess_emotes_batch(pd.Series([text]))[0]\n",
    "    sentence = Sentence(processed_text)\n",
    "    try:\n",
    "        classifier.predict([sentence])  # Batch prediction with single sentence\n",
    "        label = sentence.labels[0]\n",
    "        score = label.score\n",
    "        sentiment = label.value\n",
    "    except Exception as e:\n",
    "        print(f\"Error predicting sentiment for '{text}': {e}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Override for known negative slang\n",
    "    negative_slang = ['ass', 'trash', 'garbage', 'shit', 'awful', 'terrible', 'horrible']\n",
    "    if any(word in processed_text.lower() for word in negative_slang) and sentiment == \"POSITIVE\":\n",
    "        sentiment = \"NEGATIVE\"\n",
    "    \n",
    "    # Infer NEUTRAL if confidence is below threshold\n",
    "    if score < neutral_threshold:\n",
    "        return \"NEUTRAL\", score\n",
    "    return sentiment, score\n",
    "\n",
    "# Function to analyze a newly typed sentence with Flair\n",
    "def analyze_new_sentence(text):\n",
    "    sentiment, score = get_flair_sentiment(text)\n",
    "    print(f\"\\nAnalysis of new sentence: '{text}'\")\n",
    "    print(f\"Processed text: '{preprocess_emotes_batch(pd.Series([text]))[0]}'\")\n",
    "    print(f\"Sentiment: {sentiment}\")\n",
    "    print(f\"Confidence: {score:.4f}\")\n",
    "    return sentiment, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to initialize the worker process with the classifier\n",
    "def init_worker():\n",
    "    global classifier\n",
    "    classifier = TextClassifier.load('en-sentiment')\n",
    "    classifier.to('cpu')\n",
    "\n",
    "# Function to process a single batch with Flair batch prediction\n",
    "def process_batch(batch):\n",
    "    global classifier  # Access the classifier initialized in the worker\n",
    "    # Preprocess all comments in the batch at once\n",
    "    processed_texts = preprocess_emotes_batch(batch['comment_text'])\n",
    "    sentences = [Sentence(text) for text in processed_texts.dropna()]\n",
    "    if not sentences:\n",
    "        return pd.DataFrame()\n",
    "    try:\n",
    "        classifier.predict(sentences)\n",
    "        results = []\n",
    "        for i, row in batch.iterrows():\n",
    "            comment = row['comment_text']\n",
    "            processed_text = processed_texts.iloc[i]\n",
    "            if pd.isna(comment) or comment.strip() == '[deleted]':\n",
    "                sentiment, score = None, None\n",
    "            else:\n",
    "                sentiment = sentences[i].labels[0].value if i < len(sentences) else None\n",
    "                score = sentences[i].labels[0].score if i < len(sentences) else None\n",
    "                if score is not None and score < 0.6:\n",
    "                    sentiment = \"NEUTRAL\"\n",
    "                negative_slang = ['ass', 'trash', 'garbage', 'shit', 'awful', 'terrible', 'horrible']\n",
    "                if sentiment == \"POSITIVE\" and any(word in processed_text.lower() for word in negative_slang):\n",
    "                    sentiment = \"NEGATIVE\"\n",
    "            results.append({\n",
    "                'comment_id': row['comment_id'],\n",
    "                'comment_text': comment,\n",
    "                'sentiment': sentiment,\n",
    "                'confidence': score,\n",
    "                'comment_author': row['comment_author'],\n",
    "                'comment_time': row['comment_time'],\n",
    "                'comment_score': row['comment_score'],\n",
    "                'match_time': row['match_time'],\n",
    "                'matchday': row['matchday'],\n",
    "                'home_team': row['home_team'],\n",
    "                'away_team': row['away_team'],\n",
    "                'winner': row['winner']\n",
    "            })\n",
    "        return pd.DataFrame(results)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "# Parallel processing of batches using multiprocessing\n",
    "print(\"Processing dataset with multiprocessing...\")\n",
    "start_time = time.time()\n",
    "batch_size = 10000  # Optimized for memory and speed\n",
    "batches = [df[i:i + batch_size] for i in range(0, len(df), batch_size)]\n",
    "print(f\"Number of batches: {len(batches)}\")\n",
    "\n",
    "# Use multiprocessing Pool with worker initialization\n",
    "with mp.Pool(processes=mp.cpu_count(), initializer=init_worker) as pool:\n",
    "    results = []\n",
    "    with tqdm(total=len(batches), desc=\"Processing sentiment\", position=0, leave=True) as pbar:\n",
    "        for i, result in enumerate(pool.imap(process_batch, batches)):\n",
    "            print(f\"Processed batch {i + 1}/{len(batches)}\")  # Debug output\n",
    "            results.append(result)\n",
    "            pbar.update(1)  # Manually update the progress bar\n",
    "            time.sleep(0.1)\n",
    "\n",
    "results_df = pd.concat(results, ignore_index=True)\n",
    "print(f\"Processing completed in {time.time() - start_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_teams_vectorized(comment_series, home_team_series, away_team_series, processed_series):\n",
    "    def detect_single(comment, home_team, away_team, processed_text):\n",
    "        if pd.isna(comment):\n",
    "            return [None]\n",
    "        comment = processed_text.lower()\n",
    "        teams = []\n",
    "        home_team_lower = home_team.lower()\n",
    "        away_team_lower = away_team.lower()\n",
    "        if home_team_lower in comment or any(kw in comment for kw in [home_team_lower[:4], f\"{home_team_lower.split()[0]}\"]):\n",
    "            teams.append(home_team)\n",
    "        if away_team_lower in comment or any(kw in comment for kw in [away_team_lower[:4], f\"{away_team_lower.split()[0]}\"]):\n",
    "            teams.append(away_team)\n",
    "        \n",
    "        team_keywords = {\n",
    "            'Arsenal': ['arsenal', 'afc', 'gooners'],\n",
    "            'Aston Villa': ['aston villa', 'villa', 'avfc'],\n",
    "            'Bournemouth': ['bournemouth', 'afcb', 'cherries'],\n",
    "            'Brentford': ['brentford', 'bfc', 'bees'],\n",
    "            'Brighton & Hove Albion': ['brighton', 'bha', 'seagulls'],\n",
    "            'Chelsea': ['chelsea', 'cfc', 'blues'],\n",
    "            'Crystal Palace': ['crystal palace', 'palace', 'cpfc'],\n",
    "            'Everton': ['everton', 'efc', 'toffees'],\n",
    "            'Fulham': ['fulham', 'ffc', 'cottagers'],\n",
    "            'Ipswich Town': ['ipswich', 'itfc', 'tractor boys'],\n",
    "            'Leicester City': ['leicester', 'lcfc', 'foxes'],\n",
    "            'Liverpool': ['liverpool', 'lfc', 'reds'],\n",
    "            'Manchester City': ['manchester city', 'city', 'mcfc'],\n",
    "            'Manchester United': ['manchester united', 'united', 'man utd', 'mufc'],\n",
    "            'Newcastle United': ['newcastle', 'nufc', 'magpies'],\n",
    "            'Nottingham Forest': ['nottingham forest', 'forest', 'nffc'],\n",
    "            'Southampton': ['southampton', 'saints', 'sfc'],\n",
    "            'Tottenham Hotspur': ['tottenham', 'spurs', 'thfc'],\n",
    "            'West Ham United': ['west ham', 'whu', 'hammers'],\n",
    "            'Wolverhampton Wanderers': ['wolves', 'wwfc', 'wanderers']\n",
    "        }\n",
    "        \n",
    "        for team, keywords in team_keywords.items():\n",
    "            if any(keyword in comment for keyword in keywords):\n",
    "                if team not in teams:\n",
    "                    teams.append(team)\n",
    "        return teams if teams else [None]\n",
    "\n",
    "    return [detect_single(comment, home_team, away_team, processed) \n",
    "            for comment, home_team, away_team, processed in zip(comment_series, home_team_series, away_team_series, processed_series)]\n",
    "\n",
    "# Apply team detection\n",
    "print(\"Detecting teams...\")\n",
    "start_time = time.time()\n",
    "results_df['teams_mentioned'] = detect_teams_vectorized(results_df['comment_text'], results_df['home_team'], results_df['away_team'], results_df['processed_text'])\n",
    "results_exploded = results_df.explode('teams_mentioned')\n",
    "print(f\"Team detection completed in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "# Time-based preprocessing (parallelizable with joblib for large datasets)\n",
    "print(\"Performing time-based preprocessing...\")\n",
    "start_time = time.time()\n",
    "def process_time_chunk(chunk):\n",
    "    chunk['comment_datetime'] = pd.to_datetime(chunk['comment_time'], unit='s')\n",
    "    chunk['match_datetime'] = pd.to_datetime(chunk['match_time'], errors='coerce')\n",
    "    chunk['time_diff'] = (chunk['comment_datetime'] - chunk['match_datetime']).dt.total_seconds() / 60\n",
    "    return chunk\n",
    "\n",
    "chunk_size = 10000\n",
    "chunks = [results_exploded[i:i + chunk_size] for i in range(0, len(results_exploded), chunk_size)]\n",
    "with Parallel(n_jobs=num_processes) as parallel:\n",
    "    results_chunks = parallel(delayed(process_time_chunk)(chunk) for chunk in chunks)\n",
    "results_exploded = pd.concat(results_chunks, ignore_index=True)\n",
    "print(f\"Time preprocessing completed in {time.time() - start_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_by_team(df):\n",
    "    print(\"\\nSentiment by Team (Top 10 Mentioned):\")\n",
    "    team_sentiment = df.groupby('teams_mentioned')['sentiment'].value_counts().unstack(fill_value=0)\n",
    "    top_teams = team_sentiment.sum(axis=1).nlargest(10).index\n",
    "    team_sentiment = team_sentiment.loc[top_teams].reset_index()\n",
    "    print(tabulate(team_sentiment, headers='keys', tablefmt='grid', showindex=False))\n",
    "\n",
    "    team_sentiment_plot = team_sentiment.set_index('teams_mentioned')\n",
    "    team_sentiment_plot.plot(kind='bar', figsize=(10, 6), colormap='viridis')\n",
    "    plt.title('Sentiment Distribution by Top 10 Teams')\n",
    "    plt.xlabel('Team')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title='Sentiment')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nAverage Confidence by Team (Top 10):\")\n",
    "    confidence_by_team = df.groupby('teams_mentioned')['confidence'].mean().dropna()\n",
    "    confidence_table = confidence_by_team.loc[top_teams].reset_index()\n",
    "    confidence_table['confidence'] = confidence_table['confidence'].round(4)\n",
    "    print(tabulate(confidence_table, headers=['Team', 'Average Confidence'], tablefmt='grid', showindex=False))\n",
    "\n",
    "def analyze_by_matchday(df):\n",
    "    print(\"\\nSentiment by Matchday:\")\n",
    "    matchday_sentiment = df.groupby('matchday')['sentiment'].value_counts().unstack(fill_value=0).reset_index()\n",
    "    print(tabulate(matchday_sentiment, headers='keys', tablefmt='grid', showindex=False))\n",
    "\n",
    "    matchday_sentiment_plot = matchday_sentiment.set_index('matchday')\n",
    "    matchday_sentiment_plot.plot(kind='bar', figsize=(10, 6), colormap='plasma')\n",
    "    plt.title('Sentiment Distribution by Matchday')\n",
    "    plt.xlabel('Matchday')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.legend(title='Sentiment')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def analyze_temporal_trends(df):\n",
    "    print(\"\\nSentiment by Time Since Match Start (15-min intervals):\")\n",
    "    df['time_bin'] = pd.cut(df['time_diff'], bins=range(0, 121, 15), right=False)\n",
    "    time_sentiment = df.groupby('time_bin')['sentiment'].value_counts().unstack(fill_value=0).reset_index()\n",
    "    print(tabulate(time_sentiment, headers='keys', tablefmt='grid', showindex=False))\n",
    "\n",
    "    time_sentiment_plot = time_sentiment.set_index('time_bin')\n",
    "    time_sentiment_plot.plot(kind='bar', figsize=(10, 6), colormap='magma')\n",
    "    plt.title('Sentiment Distribution by Time Since Match Start')\n",
    "    plt.xlabel('Time Bin (minutes)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title='Sentiment')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def analyze_winner_effect(df):\n",
    "    print(\"\\nSentiment by Match Winner:\")\n",
    "    winner_sentiment = df.groupby('winner')['sentiment'].value_counts().unstack(fill_value=0).reset_index()\n",
    "    print(tabulate(winner_sentiment, headers='keys', tablefmt='grid', showindex=False))\n",
    "\n",
    "    winner_sentiment_plot = winner_sentiment.set_index('winner')\n",
    "    winner_sentiment_plot.plot(kind='bar', figsize=(10, 6), colormap='inferno')\n",
    "    plt.title('Sentiment Distribution by Match Winner')\n",
    "    plt.xlabel('Winner')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title='Sentiment')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def analyze_by_scenarios(df):\n",
    "    print(\"\\nSentiment Distribution by Scenarios:\")\n",
    "\n",
    "    # Scenario 1: Goal Proximity\n",
    "    goal_times = [15, 30, 60, 75, 90]\n",
    "    goal_window = 5\n",
    "    df['near_goal'] = df['time_diff'].apply(lambda x: any(abs(x - gt) <= goal_window for gt in goal_times))\n",
    "    print(\"\\nScenario 1: Comments Near Likely Goal Times (within 5 minutes):\")\n",
    "    goal_sentiment = df[df['near_goal'] == True]['sentiment'].value_counts().reindex(['POSITIVE', 'NEGATIVE', 'NEUTRAL'], fill_value=0).reset_index()\n",
    "    goal_sentiment.columns = ['Sentiment', 'Count']\n",
    "    print(tabulate(goal_sentiment, headers='keys', tablefmt='grid', showindex=False))\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.pie(goal_sentiment['Count'], labels=goal_sentiment['Sentiment'], autopct='%1.1f%%', colors=['green', 'red', 'gray'])\n",
    "    plt.title('Sentiment Distribution Near Goal Times')\n",
    "    plt.show()\n",
    "\n",
    "    # Scenario 2: Halftime\n",
    "    df['is_halftime'] = df['time_diff'].between(45, 60)\n",
    "    print(\"\\nScenario 2: Comments During Halftime (45-60 minutes):\")\n",
    "    halftime_sentiment = df[df['is_halftime'] == True]['sentiment'].value_counts().reindex(['POSITIVE', 'NEGATIVE', 'NEUTRAL'], fill_value=0).reset_index()\n",
    "    halftime_sentiment.columns = ['Sentiment', 'Count']\n",
    "    print(tabulate(halftime_sentiment, headers='keys', tablefmt='grid', showindex=False))\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.pie(halftime_sentiment['Count'], labels=halftime_sentiment['Sentiment'], autopct='%1.1f%%', colors=['green', 'red', 'gray'])\n",
    "    plt.title('Sentiment Distribution During Halftime')\n",
    "    plt.show()\n",
    "\n",
    "    # Scenario 3: Match Outcome\n",
    "    print(\"\\nScenario 3: Sentiment by Match Outcome (Winner):\")\n",
    "    outcome_sentiment = df.groupby('winner')['sentiment'].value_counts().unstack(fill_value=0).reset_index()\n",
    "    print(tabulate(outcome_sentiment, headers='keys', tablefmt='grid', showindex=False))\n",
    "\n",
    "    outcome_sentiment_plot = outcome_sentiment.set_index('winner')\n",
    "    outcome_sentiment_plot.plot(kind='bar', figsize=(10, 6), colormap='cividis')\n",
    "    plt.title('Sentiment Distribution by Match Winner')\n",
    "    plt.xlabel('Winner')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title='Sentiment')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Scenario 4: Matchday Trends\n",
    "    print(\"\\nScenario 4: Sentiment by Matchday (Summary):\")\n",
    "    matchday_sentiment = df.groupby('matchday')['sentiment'].value_counts().unstack(fill_value=0).reset_index()\n",
    "    print(tabulate(matchday_sentiment, headers='keys', tablefmt='grid', showindex=False))\n",
    "\n",
    "    matchday_sentiment_plot = matchday_sentiment.set_index('matchday')\n",
    "    matchday_sentiment_plot.plot(kind='bar', figsize=(10, 6), colormap='cool')\n",
    "    plt.title('Sentiment Distribution by Matchday')\n",
    "    plt.xlabel('Matchday')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.legend(title='Sentiment')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute analysis with tables and visualizations\n",
    "print(\"Overall Sentiment Distribution:\")\n",
    "overall_sentiment = results_exploded['sentiment'].value_counts().reset_index()\n",
    "overall_sentiment.columns = ['Sentiment', 'Count']\n",
    "print(tabulate(overall_sentiment, headers='keys', tablefmt='grid', showindex=False))\n",
    "\n",
    "# Visualization: Pie chart for overall sentiment\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(overall_sentiment['Count'], labels=overall_sentiment['Sentiment'], autopct='%1.1f%%', colors=['green', 'red', 'gray'])\n",
    "plt.title('Overall Sentiment Distribution')\n",
    "plt.show()\n",
    "\n",
    "analyze_by_team(results_exploded)\n",
    "analyze_by_matchday(results_exploded)\n",
    "analyze_temporal_trends(results_exploded)\n",
    "analyze_winner_effect(results_exploded)\n",
    "analyze_by_scenarios(results_exploded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze a new sentence\n",
    "new_sentence = \"Good pass Licha Brilliant from Amad in terms of the run, rounding the keeper, and finishing from a tight angle IDK what Ederson is doing (as usual) 😊\"\n",
    "analyze_new_sentence(new_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
