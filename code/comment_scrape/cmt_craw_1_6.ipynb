{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import asyncpraw\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H√†m kh·ªüi t·∫°o, h√†m extract commment, h√†m check rate limit, h√†m get all cmt c·ªßa 1 post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# üîπ H·ªó tr·ª£ asyncio trong Jupyter Notebook\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# üîπ S·ªë l∆∞·ª£ng request ƒë·ªìng th·ªùi t·ªëi ƒëa\n",
    "MAX_CONCURRENT_REQUESTS = 1\n",
    "\n",
    "# üîπ Kh·ªüi t·∫°o Reddit API client\n",
    "async def create_reddit_client():\n",
    "    return asyncpraw.Reddit(\n",
    "        client_id=\"pPEPZOQQcEgduy1bXHXaUQ\",\n",
    "        client_secret=\"v6Ta6emxFqC1hDWV6XWVPzwCNI_iQg\",\n",
    "        user_agent=\"SensitiveRaccoon1668\",\n",
    "        request_timeout=90\n",
    "    )\n",
    "\n",
    "# üîπ Ki·ªÉm tra rate limit c√≤n l·∫°i\n",
    "async def check_rate_limit(reddit):\n",
    "    try:\n",
    "        await reddit.request(\"GET\", \"/api/v1/me\")\n",
    "        remaining = reddit.auth.limits.get(\"remaining\", 60)  # M·∫∑c ƒë·ªãnh 60 n·∫øu kh√¥ng c√≥ gi√° tr·ªã\n",
    "        reset_time = reddit.auth.limits.get(\"reset_timestamp\", time.time() + 60)\n",
    "        print(f\"Remaining requests: {remaining}, Reset time: {datetime.utcfromtimestamp(reset_time).strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        return remaining, reset_time\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Kh√¥ng th·ªÉ ki·ªÉm tra rate limit: {e}\")\n",
    "        return 60, time.time() + 60  # M·∫∑c ƒë·ªãnh n·∫øu kh√¥ng l·∫•y ƒë∆∞·ª£c d·ªØ li·ªáu\n",
    "\n",
    "# üîπ H√†m l·∫•y to√†n b·ªô comment c·ªßa m·ªôt b√†i post\n",
    "async def get_all_comments_async(reddit, post_id, semaphore, max_retries=3):\n",
    "    async with semaphore:  \n",
    "        remaining, reset_time = await check_rate_limit(reddit)\n",
    "\n",
    "        if remaining < 5:  # N·∫øu c√≤n qu√° √≠t request, ch·ªù ƒë·∫øn reset\n",
    "            wait_time = reset_time - time.time() + 1\n",
    "            print(f\"‚è≥ Rate limit s·∫Øp h·∫øt, ch·ªù {wait_time:.2f}s...\")\n",
    "            await asyncio.sleep(wait_time)\n",
    "\n",
    "        for attempt in range(max_retries):  # Th·ª≠ t·ªëi ƒëa max_retries l·∫ßn\n",
    "            try:\n",
    "                print(f\"üìå [L·∫ßn {attempt+1}] ƒêang l·∫•y comment c·ªßa post: {post_id}...\")\n",
    "                submission = await reddit.submission(id=post_id)\n",
    "                await submission.comments.replace_more(limit=None)\n",
    "                comments_list = submission.comments\n",
    "                def extract_comments(comment_list):\n",
    "                    all_comments = []\n",
    "                    for comment in comment_list:\n",
    "                        all_comments.append({\n",
    "                            \"post_id\": post_id,\n",
    "                            \"comment_id\": comment.id,\n",
    "                            \"comment_text\": comment.body,\n",
    "                            \"comment_author\": comment.author.name if comment.author else \"Anonymous\",\n",
    "                            \"comment_score\": comment.score,\n",
    "                            \"comment_time\": comment.created_utc\n",
    "                        })\n",
    "                        all_comments.extend(extract_comments(comment.replies))  \n",
    "                    return all_comments\n",
    "\n",
    "                comments = extract_comments(comments_list)\n",
    "                print(f\"‚úÖ Ho√†n t·∫•t post {post_id}, l·∫•y ƒë∆∞·ª£c {len(comments)} comment.\")\n",
    "\n",
    "                # üîπ ƒêi·ªÅu ch·ªânh t·ªëc ƒë·ªô request d·ª±a v√†o remaining\n",
    "                delay = max(1, random.uniform(2, 6) / remaining)  \n",
    "                print(f\"‚è≥ Ch·ªù {delay:.2f}s tr∆∞·ªõc khi ti·∫øp t·ª•c...\")\n",
    "                await asyncio.sleep(delay)\n",
    "\n",
    "                return comments\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è L·ªói khi l·∫•y comment c·ªßa post {post_id} (L·∫ßn {attempt+1}/{max_retries}): {e}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    wait_time = 2 ** attempt  # Ch·ªù l√¢u h∆°n sau m·ªói l·∫ßn th·ª≠\n",
    "                    print(f\"üîÅ Th·ª≠ l·∫°i sau {wait_time}s...\")\n",
    "                    await asyncio.sleep(wait_time)\n",
    "                else:\n",
    "                    print(f\"‚ùå B·ªè qua post {post_id} sau {max_retries} l·∫ßn th·ª≠.\")\n",
    "                    return []\n",
    "\n",
    "# üîπ H√†m x·ª≠ l√Ω nhi·ªÅu post t·ª´ file CSV\n",
    "async def process_posts_from_csv(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "        # **L·ªçc ch·ªâ l·∫•y matchday t·ª´ 7 ƒë·∫øn 12**\n",
    "    df = df[(df[\"matchday\"] >= 1) & (df[\"matchday\"] <= 6)]\n",
    "    post_ids = df[\"submission_id\"].dropna().astype(str).tolist()\n",
    "    print(f\"üîÑ T·ªïng s·ªë tr·∫≠n ƒë·∫•u c·∫ßn c√†o: {len(post_ids)}\\n\")\n",
    "\n",
    "    reddit = await create_reddit_client()\n",
    "    semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)\n",
    "\n",
    "    tasks = [get_all_comments_async(reddit, post_id, semaphore) for post_id in post_ids]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "\n",
    "    await reddit.close()\n",
    "\n",
    "    all_comments = [comment for post_comments in results for comment in post_comments]\n",
    "    print(f\"\\nüìä T·ªïng s·ªë comment l·∫•y ƒë∆∞·ª£c: {len(all_comments)}\")\n",
    "    return all_comments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üîπ Ch·∫°y script\n",
    "csv_file = r\"D:\\Desktop\\tap tanh hoc code\\.vscode\\DAP\\Reddit_Sentiment_Analysis\\dataframe\\full_post(finished_matches_order).csv\"\n",
    "comments = asyncio.run(process_posts_from_csv(csv_file))\n",
    "\n",
    "# üîπ L∆∞u k·∫øt qu·∫£ v√†o file CSV\n",
    "df_comments = pd.DataFrame(comments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Ho√†n th√†nh! ƒê√£ l·∫•y 35722 comment v√† l∆∞u v√†o reddit_comments.csv.\n"
     ]
    }
   ],
   "source": [
    "df_comments.to_csv(\"reddit_comments.csv\", index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Ho√†n th√†nh! ƒê√£ l·∫•y {len(comments)} comment v√† l∆∞u v√†o reddit_comments.csv.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    post_id  comment_count\n",
      "0   1etwv7n           3817\n",
      "1   1eufbfh           2371\n",
      "2   1eui1ym           1374\n",
      "3   1eui26i            196\n",
      "4   1eui3pz            268\n",
      "5   1eui74j             44\n",
      "6   1eullxa            601\n",
      "7   1ev8sh0            755\n",
      "8   1evbj42           4631\n",
      "9   1ew9irk           1563\n",
      "10  1f02vo3           2139\n",
      "11  1f05t5x            829\n",
      "12  1f05yy1             49\n",
      "13  1f0627i            301\n",
      "14  1f069ds             62\n",
      "15  1f094m4           2609\n",
      "16  1f0vr01            418\n",
      "17  1f0vtuu           2162\n",
      "18  1f0yurc            757\n",
      "19  1f5kvh8           3099\n",
      "20  1f5nlii            154\n",
      "21  1f5nnr5             25\n",
      "22  1f5nve4              6\n",
      "23  1f5o3c8             88\n",
      "24  1f5oln4             34\n",
      "25  1f5rjm0            719\n",
      "26  1f6d8bc            675\n",
      "27  1f6darv           1044\n",
      "28  1f6j9hv            256\n",
      "29  1fgjren           1594\n",
      "30  1fgmi4r            443\n",
      "31  1fgmp6b             20\n",
      "32  1fgmpft            796\n",
      "33  1fgmuru             58\n",
      "34  1fgmyhe             36\n",
      "35  1fgqcnb            673\n",
      "36  1fgswhs           1174\n",
      "37  1fhbl7c           4930\n",
      "38  1fhfbbc            445\n",
      "39  1fm0ykd           1483\n",
      "40  1fm3r5j             58\n",
      "41  1fm3s6v            403\n",
      "42  1fm3vzp            405\n",
      "43  1fm44vm             18\n",
      "44  1fm70xe           1132\n",
      "45  1fmsu5s            603\n",
      "46  1fmvl6j          11120\n",
      "47  1frc7wa           1991\n",
      "48  1frey01             23\n",
      "49  1frey5d           1707\n",
      "50  1freyaz             18\n",
      "51  1frf237           1598\n",
      "52  1frfrad              6\n",
      "53  1fri7kp            914\n",
      "54  1fs3uzi            464\n",
      "55  1fs7306           5715\n",
      "56  1ft3odp            327\n",
      "69200\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "df_comments = pd.read_csv(r'D:\\Desktop\\tap tanh hoc code\\.vscode\\DAP\\reddit_comments_updated.csv')\n",
    "\n",
    "# Group by 'post_id' and count the number of comments for each post\n",
    "comment_counts = df_comments.groupby('post_id').size().reset_index(name='comment_count')\n",
    "\n",
    "# Display the result\n",
    "print(comment_counts)\n",
    "total_comment = comment_counts['comment_count'].sum()\n",
    "print(total_comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T√¨m l·∫°i nh·ªØng Post b·ªã miss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>match_id</th>\n",
       "      <th>utc_date</th>\n",
       "      <th>status</th>\n",
       "      <th>matchday</th>\n",
       "      <th>home_team_x</th>\n",
       "      <th>home_team_tla</th>\n",
       "      <th>home_team_short_name</th>\n",
       "      <th>away_team_x</th>\n",
       "      <th>away_team_tla</th>\n",
       "      <th>away_team_short_name</th>\n",
       "      <th>...</th>\n",
       "      <th>match_key</th>\n",
       "      <th>home_team_y</th>\n",
       "      <th>away_team_y</th>\n",
       "      <th>match_time</th>\n",
       "      <th>post_date</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>comments</th>\n",
       "      <th>submission_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>497410</td>\n",
       "      <td>2024-08-16 19:00:00</td>\n",
       "      <td>FINISHED</td>\n",
       "      <td>1</td>\n",
       "      <td>Manchester United</td>\n",
       "      <td>MUN</td>\n",
       "      <td>Man United</td>\n",
       "      <td>Fulham</td>\n",
       "      <td>FUL</td>\n",
       "      <td>Fulham</td>\n",
       "      <td>...</td>\n",
       "      <td>Manchester United vs Fulham</td>\n",
       "      <td>Manchester United</td>\n",
       "      <td>Fulham</td>\n",
       "      <td>2024-08-16 19:00</td>\n",
       "      <td>2024-08-16 18:55</td>\n",
       "      <td>Match Thread: Manchester United vs Fulham | En...</td>\n",
       "      <td>https://www.reddit.com/r/soccer/comments/1etwv...</td>\n",
       "      <td>175.0</td>\n",
       "      <td>3826.0</td>\n",
       "      <td>1etwv7n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>497411</td>\n",
       "      <td>2024-08-17 11:30:00</td>\n",
       "      <td>FINISHED</td>\n",
       "      <td>1</td>\n",
       "      <td>Ipswich Town</td>\n",
       "      <td>IPS</td>\n",
       "      <td>Ipswich Town</td>\n",
       "      <td>Liverpool</td>\n",
       "      <td>LIV</td>\n",
       "      <td>Liverpool</td>\n",
       "      <td>...</td>\n",
       "      <td>Ipswich Town vs Liverpool</td>\n",
       "      <td>Ipswich Town</td>\n",
       "      <td>Liverpool</td>\n",
       "      <td>2024-08-17 11:30</td>\n",
       "      <td>2024-08-17 11:18</td>\n",
       "      <td>Match Thread: Ipswich Town vs Liverpool | Engl...</td>\n",
       "      <td>https://www.reddit.com/r/soccer/comments/1eufb...</td>\n",
       "      <td>103.0</td>\n",
       "      <td>2349.0</td>\n",
       "      <td>1eufbfh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>497412</td>\n",
       "      <td>2024-08-17 14:00:00</td>\n",
       "      <td>FINISHED</td>\n",
       "      <td>1</td>\n",
       "      <td>Arsenal</td>\n",
       "      <td>ARS</td>\n",
       "      <td>Arsenal</td>\n",
       "      <td>Wolverhampton Wanderers</td>\n",
       "      <td>WOL</td>\n",
       "      <td>Wolverhampton</td>\n",
       "      <td>...</td>\n",
       "      <td>Arsenal vs Wolverhampton Wanderers</td>\n",
       "      <td>Arsenal</td>\n",
       "      <td>Wolverhampton Wanderers</td>\n",
       "      <td>2024-08-17 14:00</td>\n",
       "      <td>2024-08-17 13:43</td>\n",
       "      <td>Match Thread: Arsenal vs Wolverhampton Wandere...</td>\n",
       "      <td>https://www.reddit.com/r/soccer/comments/1eui1...</td>\n",
       "      <td>57.0</td>\n",
       "      <td>1375.0</td>\n",
       "      <td>1eui1ym</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>497413</td>\n",
       "      <td>2024-08-17 14:00:00</td>\n",
       "      <td>FINISHED</td>\n",
       "      <td>1</td>\n",
       "      <td>Everton</td>\n",
       "      <td>EVE</td>\n",
       "      <td>Everton</td>\n",
       "      <td>Brighton &amp; Hove Albion</td>\n",
       "      <td>BHA</td>\n",
       "      <td>Brighton Hove</td>\n",
       "      <td>...</td>\n",
       "      <td>Everton vs Brighton &amp; Hove Albion</td>\n",
       "      <td>Everton</td>\n",
       "      <td>Brighton &amp; Hove Albion</td>\n",
       "      <td>2024-08-17 14:00</td>\n",
       "      <td>2024-08-17 13:43</td>\n",
       "      <td>Match Thread: Everton vs Brighton &amp; Hove Albio...</td>\n",
       "      <td>https://www.reddit.com/r/soccer/comments/1eui2...</td>\n",
       "      <td>21.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>1eui26i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>497414</td>\n",
       "      <td>2024-08-17 14:00:00</td>\n",
       "      <td>FINISHED</td>\n",
       "      <td>1</td>\n",
       "      <td>Newcastle United</td>\n",
       "      <td>NEW</td>\n",
       "      <td>Newcastle</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>SOU</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>...</td>\n",
       "      <td>Newcastle United vs Southampton</td>\n",
       "      <td>Newcastle United</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>2024-08-17 14:00</td>\n",
       "      <td>2024-08-17 13:45</td>\n",
       "      <td>Match Thread: Newcastle United vs Southampton ...</td>\n",
       "      <td>https://www.reddit.com/r/soccer/comments/1eui3...</td>\n",
       "      <td>29.0</td>\n",
       "      <td>267.0</td>\n",
       "      <td>1eui3pz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   match_id             utc_date    status  matchday        home_team_x  \\\n",
       "0    497410  2024-08-16 19:00:00  FINISHED         1  Manchester United   \n",
       "1    497411  2024-08-17 11:30:00  FINISHED         1       Ipswich Town   \n",
       "2    497412  2024-08-17 14:00:00  FINISHED         1            Arsenal   \n",
       "3    497413  2024-08-17 14:00:00  FINISHED         1            Everton   \n",
       "4    497414  2024-08-17 14:00:00  FINISHED         1   Newcastle United   \n",
       "\n",
       "  home_team_tla home_team_short_name              away_team_x away_team_tla  \\\n",
       "0           MUN           Man United                   Fulham           FUL   \n",
       "1           IPS         Ipswich Town                Liverpool           LIV   \n",
       "2           ARS              Arsenal  Wolverhampton Wanderers           WOL   \n",
       "3           EVE              Everton   Brighton & Hove Albion           BHA   \n",
       "4           NEW            Newcastle              Southampton           SOU   \n",
       "\n",
       "  away_team_short_name  ...                           match_key  \\\n",
       "0               Fulham  ...         Manchester United vs Fulham   \n",
       "1            Liverpool  ...           Ipswich Town vs Liverpool   \n",
       "2        Wolverhampton  ...  Arsenal vs Wolverhampton Wanderers   \n",
       "3        Brighton Hove  ...   Everton vs Brighton & Hove Albion   \n",
       "4          Southampton  ...     Newcastle United vs Southampton   \n",
       "\n",
       "         home_team_y              away_team_y        match_time  \\\n",
       "0  Manchester United                   Fulham  2024-08-16 19:00   \n",
       "1       Ipswich Town                Liverpool  2024-08-17 11:30   \n",
       "2            Arsenal  Wolverhampton Wanderers  2024-08-17 14:00   \n",
       "3            Everton   Brighton & Hove Albion  2024-08-17 14:00   \n",
       "4   Newcastle United              Southampton  2024-08-17 14:00   \n",
       "\n",
       "          post_date                                              title  \\\n",
       "0  2024-08-16 18:55  Match Thread: Manchester United vs Fulham | En...   \n",
       "1  2024-08-17 11:18  Match Thread: Ipswich Town vs Liverpool | Engl...   \n",
       "2  2024-08-17 13:43  Match Thread: Arsenal vs Wolverhampton Wandere...   \n",
       "3  2024-08-17 13:43  Match Thread: Everton vs Brighton & Hove Albio...   \n",
       "4  2024-08-17 13:45  Match Thread: Newcastle United vs Southampton ...   \n",
       "\n",
       "                                                 url upvotes comments  \\\n",
       "0  https://www.reddit.com/r/soccer/comments/1etwv...   175.0   3826.0   \n",
       "1  https://www.reddit.com/r/soccer/comments/1eufb...   103.0   2349.0   \n",
       "2  https://www.reddit.com/r/soccer/comments/1eui1...    57.0   1375.0   \n",
       "3  https://www.reddit.com/r/soccer/comments/1eui2...    21.0    196.0   \n",
       "4  https://www.reddit.com/r/soccer/comments/1eui3...    29.0    267.0   \n",
       "\n",
       "  submission_id  \n",
       "0       1etwv7n  \n",
       "1       1eufbfh  \n",
       "2       1eui1ym  \n",
       "3       1eui26i  \n",
       "4       1eui3pz  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# üîπ T√¨m c√°c post ch∆∞a c√†o v√† ch·∫°y l·∫°i\n",
    "async def retry_missing_posts(df_goc, df_ket_qua):\n",
    "    # üî• 1. X√°c ƒë·ªãnh c√°c post_id ch∆∞a c√†o\n",
    "    post_ids_goc = set(df_goc[\"submission_id\"].dropna().astype(str))\n",
    "    post_ids_crawled = set(df_ket_qua[\"post_id\"].dropna().astype(str))\n",
    "    \n",
    "    missing_posts = list(post_ids_goc - post_ids_crawled)\n",
    "    print(f\"\\nüîÑ T·ªïng s·ªë tr·∫≠n ch∆∞a c√†o: {len(missing_posts)}\\n\")\n",
    "\n",
    "    if not missing_posts:\n",
    "        print(\"‚úÖ Kh√¥ng c√≤n post n√†o c·∫ßn c√†o l·∫°i!\")\n",
    "        return df_ket_qua  # Kh√¥ng c·∫ßn l√†m g√¨ th√™m n·∫øu t·∫•t c·∫£ ƒë√£ c√†o\n",
    "\n",
    "    # üî• 2. C√†o l·∫°i c√°c tr·∫≠n b·ªã thi·∫øu\n",
    "    reddit = await create_reddit_client()\n",
    "    semaphore = asyncio.Semaphore(2)  # Gi·ªõi h·∫°n s·ªë request ƒë·ªìng th·ªùi\n",
    "\n",
    "    tasks = {post_id: get_all_comments_async(reddit, post_id, semaphore) for post_id in missing_posts}\n",
    "    results = await asyncio.gather(*tasks.values())\n",
    "\n",
    "    await reddit.close()\n",
    "\n",
    "    # üî• 3. T·∫°o dataframe t·ª´ d·ªØ li·ªáu m·ªõi c√†o ƒë∆∞·ª£c\n",
    "    new_comments = [comment for post_comments in results if post_comments for comment in post_comments]\n",
    "    df_new = pd.DataFrame(new_comments)\n",
    "\n",
    "    # üî• 4. Merge d·ªØ li·ªáu m·ªõi v√†o dataframe c≈©\n",
    "    df_final = pd.concat([df_ket_qua, df_new], ignore_index=True)\n",
    "\n",
    "    # üî• 5. S·∫Øp x·∫øp theo th·ªùi gian\n",
    "    if \"comment_time\" in df_final.columns:\n",
    "        df_final = df_final.sort_values(by=\"comment_time\", ascending=True)\n",
    "\n",
    "    print(f\"\\nüìä T·ªïng s·ªë comment sau khi c·∫≠p nh·∫≠t: {len(df_final)}\")\n",
    "\n",
    "    return df_final\n",
    "\n",
    "# üîπ Load d·ªØ li·ªáu\n",
    "df_goc = pd.read_csv(r\"D:\\Desktop\\tap tanh hoc code\\.vscode\\DAP\\Reddit_Sentiment_Analysis\\dataframe\\full_post(finished_matches_order).csv\")\n",
    "df_goc = df_goc[(df_goc['matchday']>=1)&(df_goc['matchday']<=6)]\n",
    "df_ket_qua = pd.read_csv(r\"D:\\Desktop\\tap tanh hoc code\\.vscode\\DAP\\reddit_comments_updated.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 25)\n",
      "57\n"
     ]
    }
   ],
   "source": [
    "print(df_goc.shape)\n",
    "print(len(set(df_ket_qua['post_id'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç T·ªïng s·ªë tr·∫≠n c·∫ßn c√†o l·∫°i: {'nan'}\n",
      "\n",
      "‚úÖ ƒê√£ l∆∞u danh s√°ch post c·∫ßn c√†o v√†o 'missing_posts.csv'!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load d·ªØ li·ªáu\n",
    "df_goc = pd.read_csv(r\"D:\\Desktop\\tap tanh hoc code\\.vscode\\DAP\\Reddit_Sentiment_Analysis\\dataframe\\full_post(finished_matches_order).csv\")\n",
    "df_goc = df_goc[(df_goc['matchday'] >= 1) & (df_goc['matchday'] <= 6)]\n",
    "\n",
    "df_ket_qua = pd.read_csv(r\"D:\\Desktop\\tap tanh hoc code\\.vscode\\DAP\\reddit_comments_updated.csv\")\n",
    "\n",
    "# Chuy·ªÉn post_id v·ªÅ d·∫°ng string ƒë·ªÉ tr√°nh l·ªói khi so s√°nh\n",
    "df_goc['submission_id'] = df_goc['submission_id'].astype(str)\n",
    "df_ket_qua['post_id'] = df_ket_qua['post_id'].astype(str)\n",
    "\n",
    "# T·∫°o danh s√°ch c√°c post_id ƒë√£ c√≥ comment\n",
    "post_ids_goc = set(df_goc[\"submission_id\"].dropna())\n",
    "post_ids_crawled = set(df_ket_qua[\"post_id\"].dropna())\n",
    "\n",
    "# üî• T√¨m post_id ch∆∞a ƒë∆∞·ª£c c√†o comment\n",
    "missing_posts = post_ids_goc - post_ids_crawled\n",
    "\n",
    "print(f\"üîç T·ªïng s·ªë tr·∫≠n c·∫ßn c√†o l·∫°i: {(missing_posts)}\")\n",
    "\n",
    "# üîπ L·ªçc c√°c tr·∫≠n ch∆∞a ƒë∆∞·ª£c c√†o t·ª´ df_goc\n",
    "df_missing = df_goc[df_goc[\"submission_id\"].isin(missing_posts)]\n",
    "df_missing.to_csv(\"missing_posts.csv\", index=False)\n",
    "\n",
    "print(\"\\n‚úÖ ƒê√£ l∆∞u danh s√°ch post c·∫ßn c√†o v√†o 'missing_posts.csv'!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ T·ªïng s·ªë tr·∫≠n ƒë·∫•u c·∫ßn c√†o: 2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_23888\\2516041929.py:25: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  print(f\"Remaining requests: {remaining}, Reset time: {datetime.utcfromtimestamp(reset_time).strftime('%Y-%m-%d %H:%M:%S')}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining requests: 834.0, Reset time: 2025-03-05 06:49:57\n",
      "üìå [L·∫ßn 1] ƒêang l·∫•y comment c·ªßa post: 1fhbl7c...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x000002510CAA6CC0>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ho√†n t·∫•t post 1fhbl7c, l·∫•y ƒë∆∞·ª£c 4930 comment.\n",
      "‚è≥ Ch·ªù 1.00s tr∆∞·ªõc khi ti·∫øp t·ª•c...\n",
      "Remaining requests: 85.0, Reset time: 2025-03-05 06:49:57\n",
      "üìå [L·∫ßn 1] ƒêang l·∫•y comment c·ªßa post: 1fmvl6j...\n",
      "‚ö†Ô∏è L·ªói khi l·∫•y comment c·ªßa post 1fmvl6j (L·∫ßn 1/3): received 429 HTTP response\n",
      "üîÅ Th·ª≠ l·∫°i sau 1s...\n",
      "üìå [L·∫ßn 2] ƒêang l·∫•y comment c·ªßa post: 1fmvl6j...\n",
      "‚úÖ Ho√†n t·∫•t post 1fmvl6j, l·∫•y ƒë∆∞·ª£c 11120 comment.\n",
      "‚è≥ Ch·ªù 1.00s tr∆∞·ªõc khi ti·∫øp t·ª•c...\n",
      "\n",
      "üìä T·ªïng s·ªë comment l·∫•y ƒë∆∞·ª£c: 16050\n",
      "\n",
      "‚úÖ Ho√†n th√†nh! ƒê√£ l·∫•y 16050 comment v√† l∆∞u v√†o reddit_comments.csv.\n"
     ]
    }
   ],
   "source": [
    "async def process_posts_from_csv(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df = df[(df[\"matchday\"] >= 1) & (df[\"matchday\"] <= 6)]\n",
    "    post_ids = df[\"submission_id\"].dropna().astype(str).tolist()\n",
    "    print(f\"üîÑ T·ªïng s·ªë tr·∫≠n ƒë·∫•u c·∫ßn c√†o: {len(post_ids)}\\n\")\n",
    "\n",
    "    reddit = await create_reddit_client()\n",
    "    semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)\n",
    "\n",
    "    tasks = [get_all_comments_async(reddit, post_id, semaphore) for post_id in post_ids]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "\n",
    "    await reddit.close()\n",
    "\n",
    "    all_comments = [comment for post_comments in results for comment in post_comments]\n",
    "    print(f\"\\nüìä T·ªïng s·ªë comment l·∫•y ƒë∆∞·ª£c: {len(all_comments)}\")\n",
    "    return all_comments\n",
    "\n",
    "# üîπ Ch·∫°y script\n",
    "csv_file = r\"D:\\Desktop\\tap tanh hoc code\\.vscode\\DAP\\missing_posts.csv\"\n",
    "comments = asyncio.run(process_posts_from_csv(csv_file))\n",
    "\n",
    "# üîπ L∆∞u k·∫øt qu·∫£ v√†o file CSV\n",
    "df_missing_comments = pd.DataFrame(comments)\n",
    "df_missing_comments.to_csv(\"reddit_missing_comments.csv\", index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Ho√†n th√†nh! ƒê√£ l·∫•y {len(comments)} comment v√† l∆∞u v√†o reddit_comments.csv.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ ƒê√£ l∆∞u d·ªØ li·ªáu c·∫≠p nh·∫≠t v√†o 'reddit_comments_updated.csv'!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# üîπ G·ªôp d·ªØ li·ªáu m·ªõi v√†o dataframe c≈©\n",
    "df_final = pd.concat([df_ket_qua, df_missing_comments], ignore_index=True)\n",
    "\n",
    "# üîπ Sort theo th·ªùi gian n·∫øu c√≥ c·ªôt `comment_time`\n",
    "if \"comment_time\" in df_final.columns:\n",
    "    df_final = df_final.sort_values(by=\"comment_time\", ascending=True)\n",
    "\n",
    "# üîπ L∆∞u file sau khi c·∫≠p nh·∫≠t\n",
    "df_final.to_csv(\"reddit_comments_updated.csv\", index=False)\n",
    "print(\"\\n‚úÖ ƒê√£ l∆∞u d·ªØ li·ªáu c·∫≠p nh·∫≠t v√†o 'reddit_comments_updated.csv'!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
